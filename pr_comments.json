[{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715543","pull_request_review_id":3579982064,"id":2620715543,"node_id":"PRRC_kwDOQoxbUc6cNPIX","diff_hunk":"@@ -44,12 +44,92 @@ def forward(self, hidden_states, encoder_hidden_states, pooled_projections=None,\n             return_dict=False\n         )\n \n+# Flux Architecture Constants\n+NUM_DOUBLE_BLOCKS = 19\n+NUM_SINGLE_BLOCKS = 38\n+\n+def create_controlnet_wrapper(base_class):\n+    \"\"\"\n+    Creates a dynamic subclass of base_class (FluxModelWrapper) that accepts \n+    explicit optional arguments for ControlNet residuals.\n+    Core ML requires explicit argument names, it cannot handle a list of optional tensors well.\n+    \"\"\"\n+    \n+    # Generate explicit arguments list: c_double_0=None, ..., c_single_37=None\n+    double_args = [f\"c_double_{i}=None\" for i in range(NUM_DOUBLE_BLOCKS)]\n+    single_args = [f\"c_single_{i}=None\" for i in range(NUM_SINGLE_BLOCKS)]\n+    \n+    all_args = \", \".join(double_args + single_args)\n+    \n+    # Generate list construction code\n+    double_list = \"[\" + \", \".join([f\"c_double_{i}\" for i in range(NUM_DOUBLE_BLOCKS)]) + \"]\"\n+    single_list = \"[\" + \", \".join([f\"c_single_{i}\" for i in range(NUM_SINGLE_BLOCKS)]) + \"]\"\n+    \n+    # Define Forward Logic\n+    code = f\"\"\"\n+def forward(self, hidden_states, encoder_hidden_states, pooled_projections=None, timestep=None, img_ids=None, txt_ids=None, guidance=None, {all_args}):\n+    # Pack residuals\n+    # If any residual is provided, we assume we are in ControlNet mode\n+    # However, for tracing, we need to pass them even if None (handled by caller passing tensors)\n+    # But self.model expects lists.\n+    \n+    # Handle None inputs (if Core ML passes nothing, they might be None? No, Core ML inputs are required unless defined optional)\n+    # If defined optional, they come as nil/None? \n+    # For tracing, inputs are tensors.\n+    \n+    # Check if we have residuals to pass\n+    # Since we set defaults to None, we can check c_double_0\n+    \n+    controlnet_block_samples = None\n+    controlnet_single_block_samples = None\n+    \n+    if c_double_0 is not None:\n+        controlnet_block_samples = {double_list}\n+        \n+    if c_single_0 is not None:\n+        controlnet_single_block_samples = {single_list}\n+\n+    if self.is_flux2:\n+         return self.model(\n+            hidden_states=hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            timestep=timestep,\n+            img_ids=img_ids,\n+            txt_ids=txt_ids,\n+            guidance=guidance,\n+            controlnet_block_samples=controlnet_block_samples,\n+            controlnet_single_block_samples=controlnet_single_block_samples,\n+            return_dict=False\n+        )\n+    \n+    return self.model(\n+        hidden_states=hidden_states,\n+        encoder_hidden_states=encoder_hidden_states,\n+        pooled_projections=pooled_projections,\n+        timestep=timestep,\n+        img_ids=img_ids,\n+        txt_ids=txt_ids,\n+        guidance=guidance,\n+        controlnet_block_samples=controlnet_block_samples,\n+        controlnet_single_block_samples=controlnet_single_block_samples,\n+        return_dict=False\n+    )\n+\"\"\"\n+    scope = {}\n+    exec(code, globals(), scope)","path":"src/alloy/flux_converter.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Using exec() with globals() can introduce security risks. The code dynamically generates and executes Python code. While the inputs are controlled (NUM_DOUBLE_BLOCKS and NUM_SINGLE_BLOCKS are constants), consider refactoring this to use a more static approach, such as using functools.partial or defining the function with *args to handle variable arguments, or using getattr for dynamic parameter access. This would make the code safer and easier to maintain.","created_at":"2025-12-15T20:21:21Z","updated_at":"2025-12-15T20:21:25Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715543","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715543"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715543"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715543/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":118,"original_start_line":118,"start_side":"RIGHT","line":119,"original_line":119,"side":"RIGHT","author_association":"NONE","original_position":76,"position":76,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715562","pull_request_review_id":3579982064,"id":2620715562,"node_id":"PRRC_kwDOQoxbUc6cNPIq","diff_hunk":"@@ -0,0 +1,160 @@\n+\n+import torch\n+import torch.nn as nn\n+import coremltools as ct\n+from diffusers import FluxControlNetModel\n+import os\n+from .converter import ModelConverter\n+from .flux_converter import NUM_DOUBLE_BLOCKS, NUM_SINGLE_BLOCKS\n+from rich.console import Console\n+\n+console = Console()\n+\n+class FluxControlNetWrapper(torch.nn.Module):\n+    def __init__(self, model):\n+        super().__init__()\n+        self.model = model\n+\n+    def forward(self, controlnet_cond, hidden_states, encoder_hidden_states, timestep, img_ids, txt_ids, guidance):\n+        # Flux ControlNet Forward\n+        # Returns: FluxControlNetOutput(controlnet_block_samples=..., controlnet_single_block_samples=...)\n+        # We return a tuple of all flattened residuals\n+        \n+        out = self.model(\n+            controlnet_cond=controlnet_cond,\n+            hidden_states=hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            timestep=timestep,\n+            img_ids=img_ids,\n+            txt_ids=txt_ids,\n+            guidance=guidance,\n+            return_dict=False\n+        )\n+        # out is (block_samples, single_block_samples)\n+        block_samples = out[0]\n+        single_block_samples = out[1]\n+        \n+        # Flatten\n+        return (*block_samples, *single_block_samples)\n+\n+class FluxControlNetConverter(ModelConverter):\n+    def __init__(self, model_id, output_dir, quantization):\n+        if \"/\" not in model_id and not os.path.isfile(model_id):\n+             model_id = \"black-forest-labs/FLUX.1-Canny-dev\" # Example default\n+        super().__init__(model_id, output_dir, quantization)\n+        \n+    def convert(self):\n+        console.print(f\"[cyan]Loading Flux ControlNet:[/cyan] {self.model_id}\")\n+        try:\n+            model = FluxControlNetModel.from_pretrained(self.model_id, torch_dtype=torch.float32)\n+        except Exception as e:\n+             console.print(f\"[red]Error loading controlnet:[/red] {e}\")\n+             raise e\n+             \n+        model.eval()\n+        \n+\n+        # Determine ControlNet conditioning dimensions\n+        # Usually it matches the VAE latent dimensions but depends on the specific ControlNet\n+        # model.config doesn't always strictly define 'control_channels'.\n+        # However, Flux ControlNet from X-Labs/InstantX usually takes same shape as hidden_states? \n+        # Or it takes image latents.\n+        # Let's try to infer from the first layer.\n+        \n+        # We'll use 64x64 resolution for tracing (matches FluxConverter trace).\n+        h, w = 64, 64\n+        s = (h // 2) * (w // 2)\n+        batch_size = 1\n+        \n+        hidden_states = torch.randn(batch_size, s, in_channels).float()\n+        \n+        # Text Embeddings\n+        text_len = 256 \n+        joint_dim = model.config.joint_attention_dim # 4096\n+        encoder_hidden_states = torch.randn(batch_size, text_len, joint_dim).float()\n+        \n+        timestep = torch.tensor([1.0]).float()\n+        guidance = torch.tensor([1.0]).float()\n+        \n+        img_ids = torch.randn(s, 3).float()\n+        txt_ids = torch.randn(text_len, 3).float()\n+        \n+        # ControlNet Cond (The Hint)\n+        # Assuming it's packed latents input? \n+        # Some Flux ControlNets concat to hidden_states, so shape is (B, S, in_channels)?\n+        # Or (B, S, 3) if pixels?\n+        # Let's use `model.dtype` and a safe guess (B, S, in_channels) for now.\n+        # If it fails, user might need to adjust.\n+        # But wait, `MultiControlNetOutput` implies diffusers standard.\n+        # Diffusers standard `FluxControlNetModel` forward takes `controlnet_cond`.\n+        # The embedding layer usually maps `controlnet_cond` -> hidden_size.\n+        # Check `model.pos_embed` or `model.input_blocks`?\n+        \n+        # Let's assume input is same size as hidden_states for now (e.g. Depth map projected to latent space).\n+        controlnet_cond = torch.randn(batch_size, s, in_channels).float() \n+        \n+        example_inputs = [\n+            controlnet_cond,\n+            hidden_states,\n+            encoder_hidden_states,\n+            timestep,\n+            img_ids,\n+            txt_ids,\n+            guidance\n+        ]\n+        \n+        console.print(\"Tracing Flux ControlNet...\")\n+        wrapper = FluxControlNetWrapper(model)\n+        wrapper.eval()\n+        \n+        traced_model = torch.jit.trace(wrapper, example_inputs, strict=False)\n+        \n+        console.print(\"Converting to Core ML...\")\n+        \n+        # Inputs\n+        inputs = [\n+            ct.TensorType(name=\"controlnet_cond\", shape=controlnet_cond.shape),\n+            ct.TensorType(name=\"hidden_states\", shape=hidden_states.shape),\n+            ct.TensorType(name=\"encoder_hidden_states\", shape=encoder_hidden_states.shape),\n+            ct.TensorType(name=\"timestep\", shape=timestep.shape),\n+            ct.TensorType(name=\"img_ids\", shape=img_ids.shape),\n+            ct.TensorType(name=\"txt_ids\", shape=txt_ids.shape),\n+            ct.TensorType(name=\"guidance\", shape=guidance.shape)\n+        ]\n+        \n+        # Outputs\n+        # We must output the flattened list of residuals.\n+        # Naming MUST match the expected inputs of the Base Flux Model.\n+        # i.e. c_double_0 ... c_single_37\n+        \n+        outputs = []\n+        # Double Blocks\n+        for i in range(NUM_DOUBLE_BLOCKS):\n+            outputs.append(ct.TensorType(name=f\"c_double_{i}\"))\n+            \n+        # Single Blocks\n+        for i in range(NUM_SINGLE_BLOCKS):\n+            outputs.append(ct.TensorType(name=f\"c_single_{i}\"))\n+            \n+        ml_model_dir = os.path.join(self.output_dir, \"Flux_ControlNet.mlpackage\")\n+        \n+        ml_model = ct.convert(\n+            traced_model,\n+            inputs=inputs,\n+            outputs=outputs,\n+            compute_units=ct.ComputeUnit.ALL,\n+            minimum_deployment_target=ct.target.macOS14\n+        )\n+        \n+        if self.quantization in [\"int4\", \"4bit\", \"mixed\"]:\n+             console.print(f\"Quantizing ({self.quantization})...\")\n+             op_config = ct.optimize.coreml.OpLinearQuantizerConfig(\n+                mode=\"linear_symmetric\",\n+                weight_threshold=512\n+             )\n+             config = ct.optimize.coreml.OptimizationConfig(global_config=op_config)\n+             ml_model = ct.optimize.coreml.linear_quantize_weights(ml_model, config)\n+             \n+        ml_model.save(ml_model_dir)\n+        console.print(f\"[green]âœ“ ControlNet Saved:[/green] {ml_model_dir}\")\n+        ","path":"src/alloy/controlnet_converter.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"The new FluxControlNetConverter class and controlnet_compatible functionality lack test coverage. Given that the repository has comprehensive test coverage for other converters (e.g., test_flux_conversion.py, test_ltx_conversion.py), similar tests should be added for ControlNet conversion to ensure the new functionality works correctly, especially the critical in_channels configuration and residual input handling.","created_at":"2025-12-15T20:21:22Z","updated_at":"2025-12-15T20:21:25Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715562","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715562"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715562"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715562/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":40,"original_start_line":40,"start_side":"RIGHT","line":160,"original_line":160,"side":"RIGHT","author_association":"NONE","original_position":160,"position":160,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715593","pull_request_review_id":3579982064,"id":2620715593,"node_id":"PRRC_kwDOQoxbUc6cNPJJ","diff_hunk":"@@ -180,35 +224,97 @@ def _forward_flux(self, latents, timestep, **kwargs):\n         noise_pred = torch.from_numpy(out[\"sample\"]).to(latents.device)\n         \n         # Unpack dimensions\n-        # FluxCoreMLRunner._unpack_latents(latents, height, width, scale_factor)\n-        # Scale factor=8 usually for Flux? \n-        vae_scale_factor = 8 \n-        unpacked = FluxCoreMLRunner._unpack_latents(noise_pred, H*2*vae_scale_factor, W*2*vae_scale_factor, vae_scale_factor=1) \n-        # H, W in unpack are original image dims? \n-        # Wait, FluxCoreMLRunner._unpack_latents logic:\n-        # height = 2 * (int(height) // (vae_scale_factor * 2))\n-        # It expects \"image height/width\".\n-        # Comfy passes Latent Height/Width.\n-        # We need to reverse logic.\n-        \n-        # Re-implementing simplified unpack for arbitrary latent size:\n-        # Packed shape: (B, Patches, Channels)\n-        # Patches = (H/2 * W/2). Channels = 64 (16*4).\n-        # We want (B, 16, H, W).\n         unpacked = noise_pred.view(B, H//2, W//2, C//4, 2, 2)\n         unpacked = unpacked.permute(0, 3, 1, 4, 2, 5)\n         unpacked = unpacked.reshape(B, C, H, W)\n         \n         return unpacked\n \n+class CoreMLControlNetLoader:\n+    \"\"\"Loads a Converted ControlNet Model (.mlpackage)\"\"\"\n+    @classmethod\n+    def INPUT_TYPES(s):\n+        return {\"required\": { \n+            \"controlnet_path\": (folder_paths.get_filename_list(\"controlnet\"),)\n+        }}\n+\n+    RETURN_TYPES = (\"COREML_CONTROLNET\",)\n+    FUNCTION = \"load_controlnet\"\n+    CATEGORY = \"Alloy\"\n+\n+    def load_controlnet(self, controlnet_path):\n+        base_path = folder_paths.get_full_path(\"controlnet\", controlnet_path)\n+        print(f\"Loading Core ML ControlNet: {base_path}\")\n+        model = ct.models.MLModel(base_path)\n+        return (model,)\n+\n+class CoreMLApplyControlNet:\n+    \"\"\"Applies Core ML ControlNet to a Core ML Flux Model\"\"\"\n+    @classmethod\n+    def INPUT_TYPES(s):\n+        return {\"required\": { \n+            \"model\": (\"MODEL\",),\n+            \"controlnet\": (\"COREML_CONTROLNET\",),\n+            \"image\": (\"IMAGE\",), # Expects latent-compatible image? or Tensor\n+            \"strength\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01}),\n+        }}\n+\n+    RETURN_TYPES = (\"MODEL\",)\n+    FUNCTION = \"apply_controlnet\"\n+    CATEGORY = \"Alloy\"\n+\n+    def apply_controlnet(self, model, controlnet, image, strength):\n+        print(\"Applying Core ML ControlNet...\")\n+        wrapper = model.model\n+        if not isinstance(wrapper, CoreMLFluxWrapper):\n+            print(\"Error: Model is not a CoreMLFluxWrapper. Skipping ControlNet.\")\n+            return (model,)\n+            \n+        new_wrapper = wrapper.clone_with_residuals(wrapper.controlnet_residuals) \n+        \n+        # Pre-process image/latents?\n+        # If image is Comfy Image (1, H, W, 3) [0..1]\n+        # and we need (1, 4, H/8, W/8)?\n+        # For full correctness we need VAE Encode usually.\n+        # But here we assume user passes the correct tensor (e.g. from VAE Encode node).\n+        # VAE Encode Node returns LATENT.\n+        # ApplyControlNet input type \"IMAGE\" refers to pixels.\n+        # If we change input type to \"LATENT\", it would verify.\n+        # For flexibility, let's keep \"IMAGE\" but check type.\n+        # If tensor is (B, C, H, W), store it.\n+        \n+        if isinstance(image, dict) and \"samples\" in image:\n+             # It's a LATENT dict\n+             img_tensor = image[\"samples\"]\n+        else:\n+             # Checks if it is permuted? Comfy Image is channel last (B, H, W, C).\n+             # We convert to channel first (B, C, H, W)\n+             if len(image.shape) == 4 and image.shape[-1] in [1, 3, 4]:\n+                  img_tensor = image.permute(0, 3, 1, 2)\n+             else:\n+                  img_tensor = image\n+        \n+        new_wrapper.active_controlnets.append({\n+            \"model\": controlnet,\n+            \"image\": img_tensor, \n+            \"strength\": strength\n+        })\n+        \n+        return (comfy.model_patcher.ModelPatcher(new_wrapper, load_device=\"cpu\", offload_device=\"cpu\"),)","path":"comfyui_custom_nodes/nodes.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"The new CoreMLControlNetLoader and CoreMLApplyControlNet nodes lack test coverage. Given that the repository has tests for other ComfyUI nodes in test_comfy_nodes.py, similar test cases should be added for these new ControlNet nodes to verify their INPUT_TYPES configuration and basic loading functionality.","created_at":"2025-12-15T20:21:22Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715593","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715593"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715593"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715593/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":230,"original_start_line":233,"start_side":"RIGHT","line":300,"original_line":303,"side":"RIGHT","author_association":"NONE","original_position":289,"position":286,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715631","pull_request_review_id":3579982064,"id":2620715631,"node_id":"PRRC_kwDOQoxbUc6cNPJv","diff_hunk":"@@ -250,6 +351,20 @@ def convert_transformer(self, transformer, ml_model_dir, pbar=None):\n             # Pop pooled_projections (index 2)\n             inputs.pop(2)\n \n+        if self.controlnet_compatible:\n+            # Add ControlNet inputs\n+            dim = transformer.config.num_attention_heads * transformer.config.attention_head_dim\n+            shape = (batch_size, s, dim)\n+            \n+            for i in range(NUM_DOUBLE_BLOCKS):\n+                # Mark as optional? If we mark optional, default is 0?\n+                # coremltools: is_optional=True requires default_value?\n+                # For simplicity, let's make them optional.\n+                inputs.append(ct.TensorType(name=f\"c_double_{i}\", shape=shape, default_value=0.0))\n+                \n+            for i in range(NUM_SINGLE_BLOCKS):\n+                inputs.append(ct.TensorType(name=f\"c_single_{i}\", shape=shape, default_value=0.0))","path":"src/alloy/flux_converter.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"The default_value is set to a scalar (0.0) but the shape parameter defines a 3D tensor (batch_size, s, dim). According to Core ML Tools documentation, when providing a default_value for a tensor input, it should either be a scalar that will be broadcast or an array matching the shape. While a scalar default will be broadcast to fill the tensor, it would be clearer to use np.zeros(shape) or explicitly document that 0.0 will be broadcast across all elements of the tensor.","created_at":"2025-12-15T20:21:22Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715631","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715631"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715631"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715631/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":363,"original_start_line":363,"start_side":"RIGHT","line":366,"original_line":366,"side":"RIGHT","author_association":"NONE","original_position":165,"position":165,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715671","pull_request_review_id":3579982064,"id":2620715671,"node_id":"PRRC_kwDOQoxbUc6cNPKX","diff_hunk":"@@ -0,0 +1,160 @@\n+\n+import torch\n+import torch.nn as nn\n+import coremltools as ct\n+from diffusers import FluxControlNetModel\n+import os\n+from .converter import ModelConverter\n+from .flux_converter import NUM_DOUBLE_BLOCKS, NUM_SINGLE_BLOCKS\n+from rich.console import Console\n+\n+console = Console()\n+\n+class FluxControlNetWrapper(torch.nn.Module):\n+    def __init__(self, model):\n+        super().__init__()\n+        self.model = model\n+\n+    def forward(self, controlnet_cond, hidden_states, encoder_hidden_states, timestep, img_ids, txt_ids, guidance):\n+        # Flux ControlNet Forward\n+        # Returns: FluxControlNetOutput(controlnet_block_samples=..., controlnet_single_block_samples=...)\n+        # We return a tuple of all flattened residuals\n+        \n+        out = self.model(\n+            controlnet_cond=controlnet_cond,\n+            hidden_states=hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            timestep=timestep,\n+            img_ids=img_ids,\n+            txt_ids=txt_ids,\n+            guidance=guidance,\n+            return_dict=False\n+        )\n+        # out is (block_samples, single_block_samples)\n+        block_samples = out[0]\n+        single_block_samples = out[1]\n+        \n+        # Flatten\n+        return (*block_samples, *single_block_samples)\n+\n+class FluxControlNetConverter(ModelConverter):\n+    def __init__(self, model_id, output_dir, quantization):\n+        if \"/\" not in model_id and not os.path.isfile(model_id):\n+             model_id = \"black-forest-labs/FLUX.1-Canny-dev\" # Example default\n+        super().__init__(model_id, output_dir, quantization)\n+        \n+    def convert(self):\n+        console.print(f\"[cyan]Loading Flux ControlNet:[/cyan] {self.model_id}\")\n+        try:\n+            model = FluxControlNetModel.from_pretrained(self.model_id, torch_dtype=torch.float32)\n+        except Exception as e:\n+             console.print(f\"[red]Error loading controlnet:[/red] {e}\")\n+             raise e\n+             \n+        model.eval()\n+        \n+\n+        # Determine ControlNet conditioning dimensions\n+        # Usually it matches the VAE latent dimensions but depends on the specific ControlNet\n+        # model.config doesn't always strictly define 'control_channels'.\n+        # However, Flux ControlNet from X-Labs/InstantX usually takes same shape as hidden_states? \n+        # Or it takes image latents.\n+        # Let's try to infer from the first layer.\n+        \n+        # We'll use 64x64 resolution for tracing (matches FluxConverter trace).\n+        h, w = 64, 64\n+        s = (h // 2) * (w // 2)\n+        batch_size = 1\n+        \n+        hidden_states = torch.randn(batch_size, s, in_channels).float()","path":"src/alloy/controlnet_converter.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"The variable `in_channels` is undefined. It should be retrieved from the model configuration, similar to how it's done in flux_converter.py. Add `in_channels = model.config.in_channels` before this line.","created_at":"2025-12-15T20:21:22Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715671","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715671"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715671"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715671/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":69,"original_line":69,"side":"RIGHT","author_association":"NONE","original_position":69,"position":69,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715710","pull_request_review_id":3579982064,"id":2620715710,"node_id":"PRRC_kwDOQoxbUc6cNPK-","diff_hunk":"@@ -0,0 +1,160 @@\n+\n+import torch\n+import torch.nn as nn\n+import coremltools as ct\n+from diffusers import FluxControlNetModel\n+import os\n+from .converter import ModelConverter\n+from .flux_converter import NUM_DOUBLE_BLOCKS, NUM_SINGLE_BLOCKS\n+from rich.console import Console\n+\n+console = Console()\n+\n+class FluxControlNetWrapper(torch.nn.Module):\n+    def __init__(self, model):\n+        super().__init__()\n+        self.model = model\n+\n+    def forward(self, controlnet_cond, hidden_states, encoder_hidden_states, timestep, img_ids, txt_ids, guidance):\n+        # Flux ControlNet Forward\n+        # Returns: FluxControlNetOutput(controlnet_block_samples=..., controlnet_single_block_samples=...)\n+        # We return a tuple of all flattened residuals\n+        \n+        out = self.model(\n+            controlnet_cond=controlnet_cond,\n+            hidden_states=hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            timestep=timestep,\n+            img_ids=img_ids,\n+            txt_ids=txt_ids,\n+            guidance=guidance,\n+            return_dict=False\n+        )\n+        # out is (block_samples, single_block_samples)\n+        block_samples = out[0]\n+        single_block_samples = out[1]\n+        \n+        # Flatten\n+        return (*block_samples, *single_block_samples)\n+\n+class FluxControlNetConverter(ModelConverter):\n+    def __init__(self, model_id, output_dir, quantization):\n+        if \"/\" not in model_id and not os.path.isfile(model_id):\n+             model_id = \"black-forest-labs/FLUX.1-Canny-dev\" # Example default\n+        super().__init__(model_id, output_dir, quantization)\n+        \n+    def convert(self):\n+        console.print(f\"[cyan]Loading Flux ControlNet:[/cyan] {self.model_id}\")\n+        try:\n+            model = FluxControlNetModel.from_pretrained(self.model_id, torch_dtype=torch.float32)\n+        except Exception as e:\n+             console.print(f\"[red]Error loading controlnet:[/red] {e}\")\n+             raise e\n+             \n+        model.eval()\n+        \n+\n+        # Determine ControlNet conditioning dimensions\n+        # Usually it matches the VAE latent dimensions but depends on the specific ControlNet\n+        # model.config doesn't always strictly define 'control_channels'.\n+        # However, Flux ControlNet from X-Labs/InstantX usually takes same shape as hidden_states? \n+        # Or it takes image latents.\n+        # Let's try to infer from the first layer.\n+        \n+        # We'll use 64x64 resolution for tracing (matches FluxConverter trace).\n+        h, w = 64, 64\n+        s = (h // 2) * (w // 2)\n+        batch_size = 1\n+        \n+        hidden_states = torch.randn(batch_size, s, in_channels).float()\n+        \n+        # Text Embeddings\n+        text_len = 256 \n+        joint_dim = model.config.joint_attention_dim # 4096\n+        encoder_hidden_states = torch.randn(batch_size, text_len, joint_dim).float()\n+        \n+        timestep = torch.tensor([1.0]).float()\n+        guidance = torch.tensor([1.0]).float()\n+        \n+        img_ids = torch.randn(s, 3).float()\n+        txt_ids = torch.randn(text_len, 3).float()\n+        \n+        # ControlNet Cond (The Hint)\n+        # Assuming it's packed latents input? \n+        # Some Flux ControlNets concat to hidden_states, so shape is (B, S, in_channels)?\n+        # Or (B, S, 3) if pixels?\n+        # Let's use `model.dtype` and a safe guess (B, S, in_channels) for now.\n+        # If it fails, user might need to adjust.\n+        # But wait, `MultiControlNetOutput` implies diffusers standard.\n+        # Diffusers standard `FluxControlNetModel` forward takes `controlnet_cond`.\n+        # The embedding layer usually maps `controlnet_cond` -> hidden_size.\n+        # Check `model.pos_embed` or `model.input_blocks`?\n+        \n+        # Let's assume input is same size as hidden_states for now (e.g. Depth map projected to latent space).\n+        controlnet_cond = torch.randn(batch_size, s, in_channels).float() ","path":"src/alloy/controlnet_converter.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"The variable `in_channels` is undefined. It should be retrieved from the model configuration. Add `in_channels = model.config.in_channels` before this line.","created_at":"2025-12-15T20:21:23Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715710","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715710"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715710"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715710/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":94,"original_line":94,"side":"RIGHT","author_association":"NONE","original_position":94,"position":94,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715747","pull_request_review_id":3579982064,"id":2620715747,"node_id":"PRRC_kwDOQoxbUc6cNPLj","diff_hunk":"@@ -74,93 +74,73 @@ def load_coreml_model(self, model_path, num_frames):\n         return (comfy.model_patcher.ModelPatcher(wrapper, load_device=\"cpu\", offload_device=\"cpu\"),)\n \n \n+        return unpacked\n+\n+        return unpacked\n+","path":"comfyui_custom_nodes/nodes.py","commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"These two unreachable return statements are orphaned code that will never execute. They appear after the class definition for CoreMLWanVideoLoader and before CoreMLFluxWrapper. Remove these duplicate return statements.\n```suggestion\n\n```","created_at":"2025-12-15T20:21:23Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715747","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715747"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715747"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715747/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":77,"start_side":"RIGHT","line":null,"original_line":80,"side":"RIGHT","author_association":"NONE","original_position":7,"position":1,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715807","pull_request_review_id":3579982064,"id":2620715807,"node_id":"PRRC_kwDOQoxbUc6cNPMf","diff_hunk":"@@ -180,35 +224,97 @@ def _forward_flux(self, latents, timestep, **kwargs):\n         noise_pred = torch.from_numpy(out[\"sample\"]).to(latents.device)\n         \n         # Unpack dimensions\n-        # FluxCoreMLRunner._unpack_latents(latents, height, width, scale_factor)\n-        # Scale factor=8 usually for Flux? \n-        vae_scale_factor = 8 \n-        unpacked = FluxCoreMLRunner._unpack_latents(noise_pred, H*2*vae_scale_factor, W*2*vae_scale_factor, vae_scale_factor=1) \n-        # H, W in unpack are original image dims? \n-        # Wait, FluxCoreMLRunner._unpack_latents logic:\n-        # height = 2 * (int(height) // (vae_scale_factor * 2))\n-        # It expects \"image height/width\".\n-        # Comfy passes Latent Height/Width.\n-        # We need to reverse logic.\n-        \n-        # Re-implementing simplified unpack for arbitrary latent size:\n-        # Packed shape: (B, Patches, Channels)\n-        # Patches = (H/2 * W/2). Channels = 64 (16*4).\n-        # We want (B, 16, H, W).\n         unpacked = noise_pred.view(B, H//2, W//2, C//4, 2, 2)\n         unpacked = unpacked.permute(0, 3, 1, 4, 2, 5)\n         unpacked = unpacked.reshape(B, C, H, W)\n         \n         return unpacked\n \n+class CoreMLControlNetLoader:\n+    \"\"\"Loads a Converted ControlNet Model (.mlpackage)\"\"\"\n+    @classmethod\n+    def INPUT_TYPES(s):","path":"comfyui_custom_nodes/nodes.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Class methods or methods of a type deriving from type should have 'cls', rather than 's', as their first parameter.\n```suggestion\n    def INPUT_TYPES(cls):\n```","created_at":"2025-12-15T20:21:23Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715807","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715807"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715807"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715807/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":233,"original_line":236,"side":"RIGHT","author_association":"NONE","original_position":222,"position":219,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715866","pull_request_review_id":3579982064,"id":2620715866,"node_id":"PRRC_kwDOQoxbUc6cNPNa","diff_hunk":"@@ -180,35 +224,97 @@ def _forward_flux(self, latents, timestep, **kwargs):\n         noise_pred = torch.from_numpy(out[\"sample\"]).to(latents.device)\n         \n         # Unpack dimensions\n-        # FluxCoreMLRunner._unpack_latents(latents, height, width, scale_factor)\n-        # Scale factor=8 usually for Flux? \n-        vae_scale_factor = 8 \n-        unpacked = FluxCoreMLRunner._unpack_latents(noise_pred, H*2*vae_scale_factor, W*2*vae_scale_factor, vae_scale_factor=1) \n-        # H, W in unpack are original image dims? \n-        # Wait, FluxCoreMLRunner._unpack_latents logic:\n-        # height = 2 * (int(height) // (vae_scale_factor * 2))\n-        # It expects \"image height/width\".\n-        # Comfy passes Latent Height/Width.\n-        # We need to reverse logic.\n-        \n-        # Re-implementing simplified unpack for arbitrary latent size:\n-        # Packed shape: (B, Patches, Channels)\n-        # Patches = (H/2 * W/2). Channels = 64 (16*4).\n-        # We want (B, 16, H, W).\n         unpacked = noise_pred.view(B, H//2, W//2, C//4, 2, 2)\n         unpacked = unpacked.permute(0, 3, 1, 4, 2, 5)\n         unpacked = unpacked.reshape(B, C, H, W)\n         \n         return unpacked\n \n+class CoreMLControlNetLoader:\n+    \"\"\"Loads a Converted ControlNet Model (.mlpackage)\"\"\"\n+    @classmethod\n+    def INPUT_TYPES(s):\n+        return {\"required\": { \n+            \"controlnet_path\": (folder_paths.get_filename_list(\"controlnet\"),)\n+        }}\n+\n+    RETURN_TYPES = (\"COREML_CONTROLNET\",)\n+    FUNCTION = \"load_controlnet\"\n+    CATEGORY = \"Alloy\"\n+\n+    def load_controlnet(self, controlnet_path):\n+        base_path = folder_paths.get_full_path(\"controlnet\", controlnet_path)\n+        print(f\"Loading Core ML ControlNet: {base_path}\")\n+        model = ct.models.MLModel(base_path)\n+        return (model,)\n+\n+class CoreMLApplyControlNet:\n+    \"\"\"Applies Core ML ControlNet to a Core ML Flux Model\"\"\"\n+    @classmethod\n+    def INPUT_TYPES(s):","path":"comfyui_custom_nodes/nodes.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Class methods or methods of a type deriving from type should have 'cls', rather than 's', as their first parameter.","created_at":"2025-12-15T20:21:24Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715866","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715866"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715866"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715866/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":251,"original_line":254,"side":"RIGHT","author_association":"NONE","original_position":240,"position":237,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715895","pull_request_review_id":3579982064,"id":2620715895,"node_id":"PRRC_kwDOQoxbUc6cNPN3","diff_hunk":"@@ -0,0 +1,160 @@\n+\n+import torch\n+import torch.nn as nn","path":"src/alloy/controlnet_converter.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Import of 'nn' is not used.","created_at":"2025-12-15T20:21:24Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715895","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715895"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715895"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715895/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":3,"original_line":3,"side":"RIGHT","author_association":"NONE","original_position":3,"position":3,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715926","pull_request_review_id":3579982064,"id":2620715926,"node_id":"PRRC_kwDOQoxbUc6cNPOW","diff_hunk":"@@ -74,93 +74,73 @@ def load_coreml_model(self, model_path, num_frames):\n         return (comfy.model_patcher.ModelPatcher(wrapper, load_device=\"cpu\", offload_device=\"cpu\"),)\n \n \n+        return unpacked\n+\n+        return unpacked\n+\n class CoreMLFluxWrapper(torch.nn.Module):\n     \"\"\"Adapts Flux Core ML model to ComfyUI's sampling interface\"\"\"\n-    def __init__(self, model_path):\n+    def __init__(self, model_path, coreml_model=None):\n         super().__init__()\n         # Load Core ML model\n-        self.coreml_model = ct.models.MLModel(model_path)\n+        if coreml_model:\n+            self.coreml_model = coreml_model\n+        else:\n+            self.coreml_model = ct.models.MLModel(model_path)\n         \n-        # Configuration (minimal needed for samplers)\n+        # Configuration\n         self.latent_format = comfy.latent_formats.SDXL() # Dummy default\n         self.adm_channels = 0\n+        self.controlnet_residuals = None\n+        self.active_controlnets = [] # List of dicts: {model, image, strength}\n         \n+    def clone_with_residuals(self, residuals):\n+        \"\"\"Create a shallow copy\"\"\"\n+        new_wrapper = CoreMLFluxWrapper(None, coreml_model=self.coreml_model)\n+        new_wrapper.latent_format = self.latent_format\n+        new_wrapper.adm_channels = self.adm_channels\n+        new_wrapper.controlnet_residuals = residuals\n+        new_wrapper.active_controlnets = list(self.active_controlnets) # Shallow copy list\n+        return new_wrapper\n+\n     def forward(self, x, timestep, **kwargs):\n         \"\"\"\n         Adapts standard UNet-style inputs to Flux Core ML packed inputs.\n-        x: Latents (B, C, H, W)\n-        timestep: Tensor (B,)\n-        kwargs: \"transformer_options\", \"context\", etc.\n         \"\"\"\n-        # Report progress if available\n+        # Report progress logic...\n         transformer_options = kwargs.get(\"transformer_options\", {})\n         if hasattr(comfy.utils, 'ProgressBar'):\n-            # Try to update ComfyUI progress\n-            try:\n+             try:\n                 import comfy.model_management as mm\n                 if hasattr(mm, 'throw_exception_if_processing_interrupted'):\n                     mm.throw_exception_if_processing_interrupted()\n-            except:\n+             except:","path":"comfyui_custom_nodes/nodes.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"Except block directly handles BaseException.","created_at":"2025-12-15T20:21:24Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715926","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715926"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715926"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715926/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":114,"original_line":117,"side":"RIGHT","author_association":"NONE","original_position":54,"position":51,"subject_type":"line"},{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715952","pull_request_review_id":3579982064,"id":2620715952,"node_id":"PRRC_kwDOQoxbUc6cNPOw","diff_hunk":"@@ -74,93 +74,73 @@ def load_coreml_model(self, model_path, num_frames):\n         return (comfy.model_patcher.ModelPatcher(wrapper, load_device=\"cpu\", offload_device=\"cpu\"),)\n \n \n+        return unpacked\n+\n+        return unpacked\n+\n class CoreMLFluxWrapper(torch.nn.Module):\n     \"\"\"Adapts Flux Core ML model to ComfyUI's sampling interface\"\"\"\n-    def __init__(self, model_path):\n+    def __init__(self, model_path, coreml_model=None):\n         super().__init__()\n         # Load Core ML model\n-        self.coreml_model = ct.models.MLModel(model_path)\n+        if coreml_model:\n+            self.coreml_model = coreml_model\n+        else:\n+            self.coreml_model = ct.models.MLModel(model_path)\n         \n-        # Configuration (minimal needed for samplers)\n+        # Configuration\n         self.latent_format = comfy.latent_formats.SDXL() # Dummy default\n         self.adm_channels = 0\n+        self.controlnet_residuals = None\n+        self.active_controlnets = [] # List of dicts: {model, image, strength}\n         \n+    def clone_with_residuals(self, residuals):\n+        \"\"\"Create a shallow copy\"\"\"\n+        new_wrapper = CoreMLFluxWrapper(None, coreml_model=self.coreml_model)\n+        new_wrapper.latent_format = self.latent_format\n+        new_wrapper.adm_channels = self.adm_channels\n+        new_wrapper.controlnet_residuals = residuals\n+        new_wrapper.active_controlnets = list(self.active_controlnets) # Shallow copy list\n+        return new_wrapper\n+\n     def forward(self, x, timestep, **kwargs):\n         \"\"\"\n         Adapts standard UNet-style inputs to Flux Core ML packed inputs.\n-        x: Latents (B, C, H, W)\n-        timestep: Tensor (B,)\n-        kwargs: \"transformer_options\", \"context\", etc.\n         \"\"\"\n-        # Report progress if available\n+        # Report progress logic...\n         transformer_options = kwargs.get(\"transformer_options\", {})\n         if hasattr(comfy.utils, 'ProgressBar'):\n-            # Try to update ComfyUI progress\n-            try:\n+             try:\n                 import comfy.model_management as mm\n                 if hasattr(mm, 'throw_exception_if_processing_interrupted'):\n                     mm.throw_exception_if_processing_interrupted()\n-            except:\n+             except:","path":"comfyui_custom_nodes/nodes.py","commit_id":"1a4d1487d96f6021073d4fbf15a66900f9380b00","original_commit_id":"784caa952187f007864a6427b1a94cf5c773bcc1","user":{"login":"Copilot","id":175728472,"node_id":"BOT_kgDOCnlnWA","avatar_url":"https://avatars.githubusercontent.com/in/946600?v=4","gravatar_id":"","url":"https://api.github.com/users/Copilot","html_url":"https://github.com/apps/copilot-pull-request-reviewer","followers_url":"https://api.github.com/users/Copilot/followers","following_url":"https://api.github.com/users/Copilot/following{/other_user}","gists_url":"https://api.github.com/users/Copilot/gists{/gist_id}","starred_url":"https://api.github.com/users/Copilot/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Copilot/subscriptions","organizations_url":"https://api.github.com/users/Copilot/orgs","repos_url":"https://api.github.com/users/Copilot/repos","events_url":"https://api.github.com/users/Copilot/events{/privacy}","received_events_url":"https://api.github.com/users/Copilot/received_events","type":"Bot","user_view_type":"public","site_admin":false},"body":"'except' clause does nothing but pass and there is no explanatory comment.","created_at":"2025-12-15T20:21:24Z","updated_at":"2025-12-15T20:21:26Z","html_url":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715952","pull_request_url":"https://api.github.com/repos/hybridindie/alloy/pulls/1","_links":{"self":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715952"},"html":{"href":"https://github.com/hybridindie/alloy/pull/1#discussion_r2620715952"},"pull_request":{"href":"https://api.github.com/repos/hybridindie/alloy/pulls/1"}},"reactions":{"url":"https://api.github.com/repos/hybridindie/alloy/pulls/comments/2620715952/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"start_line":null,"original_start_line":null,"start_side":null,"line":114,"original_line":117,"side":"RIGHT","author_association":"NONE","original_position":54,"position":51,"subject_type":"line"}]